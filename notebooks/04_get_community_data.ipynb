{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Study Data Retrieval\n",
    "\n",
    "This Jupyter Notebook is intended to provide a deeper understanding of the community behind GAP distributed through GitHub, by studying the members developing, releasing and collaborating on GAP packages on GitHub, to gather valuable information on their collaboration trends and patterns. In the interest of privacy, the real values of contributor usernames are hashed upon extraction. The hash value is then the variable used to compute and generate statistical data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules and libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "from github import Repository\n",
    "\n",
    "# Get current working directory and append parent directory for module imports\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import modules from other project scripts\n",
    "from data_constants import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing GitHub API Calls\n",
    "\n",
    "The process of connecting to GitHub and verifying the user's GitHub token is done by storing the access token as an environment variable. The function for getting the token is imported from the utils file in the project. As the API has a call limit of 5000 calls per hour, the capacity and remaining calls available, as well as the reset time, is tracked below. When the user runs out of API calls, the program will sleep until the limit is renewed, and then resume the job it was completing at the time the limit ran out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the rate limit for GitHub compared to calls used, and see when the limit will reset\n",
    "# If there are less than 100 API calls left, the program will sleep until the API limit renews.\n",
    "remaining_requests, request_limit = g.rate_limiting\n",
    "print(f\"Request limit for API Calls: {request_limit}\")\n",
    "print(f\"Remaining requests for API Calls: {remaining_requests}\")\n",
    "\n",
    "limit_reset_time = g.rate_limiting_resettime\n",
    "reset_time = datetime.fromtimestamp(limit_reset_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f\"Reset time for API Calls: {reset_time}\")\n",
    "\n",
    "threshold = 100\n",
    "if remaining_requests <= threshold:\n",
    "    wait_until_reset(reset_time)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying the community\n",
    "\n",
    "Several variables related to autors and collaborations can provide valuable input on how the community behind GAP functions, and what dependencies might exist. Further investigating the frequency of contributions, who contributes to what and where connections are made yields an understanding of who the people behind the GAP packages are, how the collaborate and what the trends point to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables for the Jupyter Notebook\n",
    "org = g.get_organization(ORG_NAME_PACKAGES)\n",
    "repos = org.get_repos(type=\"public\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to Retrieve Community Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_username(author_name: str) -> str:\n",
    "    \"\"\"Hashes the author name upon retrieval, using the SHA-256 algorithm.\n",
    "\n",
    "    Args:\n",
    "        author_name (str): The author name to be hashed.\n",
    "\n",
    "    Returns:\n",
    "        str: The hash value of the author name.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(author_name.encode()).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commits_by_contributor(repo: Repository, contributors_set: set, threshold_date: datetime, inactive_contributors: dict) -> None:\n",
    "    \"\"\"Get the commits made by each contributor since the given threshold date and identify inactive contributors.\n",
    "\n",
    "    Args:\n",
    "        repo (Repository): The GitHub repository to get the commits from.\n",
    "        contributors_set (set): Hash values representing the contributors.\n",
    "        threshold_date (datetime): The threshold date to filter commits.\n",
    "        inactive_contributors (dict): Inactive contributors and their latest contribution date.\n",
    "    \"\"\"\n",
    "    for contributor_hash in contributors_set:\n",
    "        try:\n",
    "            # Get commits for each contributor\n",
    "            commits = repo.get_commits(since=threshold_date, author=contributor_hash)\n",
    "            for commit in commits:\n",
    "                if commit.author is not None:  # Check if commit.author is not None\n",
    "                    commit_timestamp = None\n",
    "                    if hasattr(commit.author, 'date'):\n",
    "                        commit_timestamp = commit.author.date\n",
    "                    elif hasattr(commit.author, 'created_at'):\n",
    "                        commit_timestamp = commit.author.created_at\n",
    "\n",
    "                    if commit_timestamp is not None and commit_timestamp < threshold_date:\n",
    "                        inactive_contributors[contributor_hash] = commit_timestamp\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing {repo.name}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_contributors(repos: Repository, threshold_months=24) -> tuple:\n",
    "    \"\"\"Get the numbers of GitHub GAP repository authors, authors who are also submitters, number of repos each author contributed to,\n",
    "    authors who are also submitters and data on what authors interacted with what issue submitters. Also, identify inactive contributors.\n",
    "\n",
    "    Args:\n",
    "        repos (Repository): List of GitHub repositories.\n",
    "        threshold_months (int, optional): Threshold in months to identify inactive contributors. Defaults to 6.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A set of hash values for all users that are authors,\n",
    "            a set of hash values for all users that are issue submitters,\n",
    "            a dict with showing how many repositories an author contributed to,\n",
    "            a set of hash values for users who are authors and submitters,\n",
    "            a dict containing authors and what issue submitters interacted with their repos\n",
    "            and a dict containing inactive contributors and their latest contribution date.\n",
    "    \"\"\"\n",
    "    all_authors = set()\n",
    "    all_submitters = set()\n",
    "    authors_submitters = set()\n",
    "    author_repo_counts = {}\n",
    "    authors_contributed_together = {}\n",
    "    inactive_contributors = {}\n",
    "    today = datetime.today()\n",
    "    threshold_date = today - timedelta(days=threshold_months * 30)\n",
    "\n",
    "    for repo in repos:\n",
    "        # Get all authors and their contribution count\n",
    "        contributors = repo.get_contributors()\n",
    "        contributors_set = set(hash_username(contributor.login) for contributor in contributors)\n",
    "        all_authors.update(contributors_set)\n",
    "\n",
    "        for contributor_hash in contributors_set:\n",
    "            author_repo_counts[contributor_hash] = author_repo_counts.get(contributor_hash, 0) + 1\n",
    "\n",
    "        # Calculate the date threshold for inactive contributors\n",
    "        threshold_date = today - timedelta(days=threshold_months * 30)\n",
    "\n",
    "        # Get inactive contributors based on threshold\n",
    "        get_commits_by_contributor(repo, contributors_set, threshold_date, inactive_contributors)\n",
    "\n",
    "        # Get all submitters for the repo\n",
    "        issues = repo.get_issues(state=\"all\")        \n",
    "        submitters_in_repo = set(hash_username(issue.user.login) for issue in issues)\n",
    "        all_submitters.update(submitters_in_repo)\n",
    "\n",
    "        # Get all interactions\n",
    "        for submitter in submitters_in_repo:\n",
    "            for contributor_hash in contributors_set:\n",
    "                if submitter != contributor_hash:\n",
    "                    if contributor_hash not in authors_contributed_together:\n",
    "                        authors_contributed_together[contributor_hash] = []\n",
    "                    if submitter not in authors_contributed_together[contributor_hash]:\n",
    "                        authors_contributed_together[contributor_hash].append(submitter)\n",
    "\n",
    "    # Get all authors and submitters\n",
    "    authors_submitters = all_submitters.intersection(all_authors)\n",
    "\n",
    "    return all_authors, all_submitters, author_repo_counts, authors_submitters, inactive_contributors, authors_contributed_together\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get and Export Community Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the functions and unpack the tuples to store the data\n",
    "all_authors, all_submitters, author_repo_counts, author_submitters, inactive_contributors, authors_contributed_together = community_contributors(repos)\n",
    "\n",
    "# Export collected data to JSON file to store them for later use and better overview\n",
    "data_folder = \"collected_data\"\n",
    "data = {\n",
    "    'authors': list(all_authors),\n",
    "    'submitters': list(all_submitters),\n",
    "    'author_repo_counts': author_repo_counts,\n",
    "    'author_submitters': list(author_submitters),\n",
    "    'inactive_contributors': inactive_contributors,\n",
    "    'interactions': authors_contributed_together\n",
    "}\n",
    "\n",
    "# Create a file path for the JSON file, and add it to the data folder\n",
    "file_path = os.path.join(data_folder, \"community_data.json\")\n",
    "\n",
    "# Write the data to the JSON file\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Community data has been exported to the 'community_data.json' file in the 'collected_data' folder.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
