{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Study Data Retrieval\n",
    "\n",
    "This Jupyter Notebook is intended to provide a deeper understanding of the community behind GAP distributed through GitHub, by studying the members developing, releasing and collaborating on GAP packages on GitHub, to gather valuable information on their collaboration trends and patterns. In the interest of privacy, the real values of contributor usernames are hashed upon extraction. The hash value is then the variable used to compute and generate statistical data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules and libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from github import Repository, RateLimitExceededException\n",
    "\n",
    "# Get current working directory and append parent directory for module imports\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import modules from other project scripts\n",
    "from data_constants import *\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying the community\n",
    "\n",
    "Several variables related to autors and collaborations can provide valuable input on how the community behind GAP functions, and what dependencies might exist. Further investigating the frequency of contributions, who contributes to what and where connections are made yields an understanding of who the people behind the GAP packages are, how the collaborate and what the trends point to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to Retrieve Community Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_username(author_name: str) -> str:\n",
    "    \"\"\"Hashes the author name upon retrieval, using the SHA-256 algorithm.\n",
    "\n",
    "    Args:\n",
    "        author_name (str): The author name to be hashed.\n",
    "\n",
    "    Returns:\n",
    "        str: The hash value of the author name.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(author_name.encode()).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commits_by_contributor(repo: Repository, contributors: set, inactive_contributors: dict) -> None:\n",
    "    \"\"\"Get the commits made by each contributor since the given threshold date and identify inactive contributors.\n",
    "\n",
    "    Args:\n",
    "        repo (Repository): The GitHub repository to get the commits from.\n",
    "        contributors (set): GitHub user objects representing the contributors.\n",
    "        inactive_contributors (dict): Inactive contributors and their latest contribution date.\n",
    "    \"\"\"\n",
    "    # Calculate the date threshold for inactive contributors\n",
    "    threshold_date = datetime.today() - relativedelta(months=12)\n",
    "\n",
    "    for contributor in contributors:\n",
    "        try:\n",
    "            # Get commits for each contributor\n",
    "            commits = repo.get_commits(since=threshold_date, author=contributor)\n",
    "            for commit in commits:\n",
    "                commit_date = commit.author.created_at\n",
    "                if commit_date is not None and commit_date < threshold_date:\n",
    "                    contributor_hash = hash_username(contributor.login)\n",
    "                    inactive_contributors[contributor_hash] = commit_date.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing {repo.name}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_contributors(repos: Repository) -> tuple:\n",
    "    \"\"\"Get the numbers of GitHub GAP repository authors, authors who are also submitters, number of repos each author contributed to,\n",
    "    authors who are also submitters and data on what authors interacted with what issue submitters. Also, identify inactive contributors.\n",
    "\n",
    "    Args:\n",
    "        repos (Repository): List of GitHub repositories.\n",
    "\n",
    "    Returns:\n",
    "        all_authors (set): Hash values for all users that are authors.\n",
    "        all_submitters (set): Hash values for all users that are issue submitters.\n",
    "        author_repo_counts (dict): Number of repositories an author contributed to.\n",
    "        authors_submitters (set): Hash values for users who are both authors and submitters.\n",
    "        authors_contributed_together (dict): Authors and what issue submitters interacted with their repos.\n",
    "        inactive_contributors (dict): Inactive contributors and their latest contribution date.\n",
    "        first_commit_by_author (dict): The first commit date for each contributor to the repo.\n",
    "    \"\"\"\n",
    "    all_authors = set()\n",
    "    all_submitters = set()\n",
    "    authors_submitters = set()\n",
    "    author_repo_counts = {}\n",
    "    authors_contributed_together = {}\n",
    "    inactive_contributors = {}\n",
    "    first_commit_by_author = {}\n",
    "\n",
    "    for repo in repos:\n",
    "        # Get all authors and their contribution count\n",
    "        contributors = repo.get_contributors()\n",
    "        for contributor in contributors:\n",
    "            contributor_hash = hash_username(contributor.login)\n",
    "            all_authors.add(contributor_hash)\n",
    "            author_repo_counts[contributor_hash] = author_repo_counts.get(contributor_hash, 0) + 1\n",
    "\n",
    "        # Get inactive contributors based on the given threshold\n",
    "        get_commits_by_contributor(repo, contributors, inactive_contributors)\n",
    "\n",
    "        # Get the first commit date for each contributor to the repo\n",
    "        for contributor in contributors:\n",
    "            try:\n",
    "                commits = repo.get_commits(author=contributor)\n",
    "                if commits.totalCount > 0:\n",
    "                    first_commit_date = commits[0].author.created_at\n",
    "                    contributor_hash = hash_username(contributor.login)\n",
    "                    first_commit_by_author[contributor_hash] = first_commit_date.strftime(\"%d-%m-%Y\")\n",
    "                else:\n",
    "                    first_commit_by_author[contributor_hash] = \"No commits\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error while getting first commit for {contributor.login} in {repo.name}: {e}\")\n",
    "\n",
    "        # Get all submitters for the repo\n",
    "        issues = repo.get_issues(state=\"all\")\n",
    "        submitters_in_repo = set(hash_username(issue.user.login) for issue in issues)\n",
    "        all_submitters.update(submitters_in_repo)\n",
    "\n",
    "        # Get all interactions\n",
    "        for submitter in submitters_in_repo:\n",
    "            for contributor in contributors:\n",
    "                contributor_hash = hash_username(contributor.login)\n",
    "                if submitter != contributor_hash:\n",
    "                    if contributor_hash not in authors_contributed_together:\n",
    "                        authors_contributed_together[contributor_hash] = []\n",
    "                    if submitter not in authors_contributed_together[contributor_hash]:\n",
    "                        authors_contributed_together[contributor_hash].append(submitter)\n",
    "\n",
    "    # Get all authors and submitters\n",
    "    authors_submitters = all_submitters.intersection(all_authors)\n",
    "\n",
    "    return all_authors, all_submitters, author_repo_counts, authors_submitters, inactive_contributors, authors_contributed_together, first_commit_by_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_community_data() -> None:\n",
    "    \"\"\"Export the community data to a JSON file, while instructing the program to sleep for the\n",
    "    duration of the time it takes for the GitHub API calls limit to reset in the event that it runs out.\n",
    "\n",
    "    Args:\n",
    "        None.\n",
    "        \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # Define organisation and repos\n",
    "            org = g.get_organization(ORG_NAME_PACKAGES)\n",
    "            repos = org.get_repos(type=\"public\")\n",
    "\n",
    "            # Get the data by calling the function and store it appropriately\n",
    "            all_authors, all_submitters, author_repo_counts, author_submitters, inactive_contributors, authors_contributed_together, first_commit_by_author = community_contributors(repos)\n",
    "            data = {\n",
    "                'authors': list(all_authors),\n",
    "                'submitters': list(all_submitters),\n",
    "                'author_repo_counts': author_repo_counts,\n",
    "                'author_submitters': list(author_submitters),\n",
    "                'inactive_contributors': inactive_contributors,\n",
    "                'interactions': authors_contributed_together,\n",
    "                'first_commit_by_author': first_commit_by_author\n",
    "            }\n",
    "\n",
    "            file_path = os.path.join(\"collected_data\", \"community_data.json\")\n",
    "\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "            print(\"Community data has been exported to the 'community_data.json' file in the 'collected_data' folder.\")\n",
    "            break\n",
    "\n",
    "        except RateLimitExceededException:\n",
    "            remaining_requests, _ = g.rate_limiting\n",
    "            reset_time = g.rate_limiting_resettime\n",
    "            if remaining_requests < 100:\n",
    "                wait_until_reset(reset_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get and Export Community Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to export the data\n",
    "export_community_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
