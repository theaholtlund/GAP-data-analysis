{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Retrieved Data\n",
    "\n",
    "This Jupyter Notebook is intended for closer examination and subsequent validation of the data that has been generated by the previous ones. Beyond having access to the data, the real value from a management perspective lies in analysing, comparing and constrasting the data, automating the process of pointing out findings that may be of significance in the redistribution process. Combining different data types from various sources, this notebook points to the noteworthy findings of the data extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules and libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from ydata_profiling import ProfileReport\n",
    "from packaging import version\n",
    "from github import Repository\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get current working directory and append parent directory for module imports\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import modules from other project scripts\n",
    "from data_constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing GitHub API Calls\n",
    "\n",
    "The process of connecting to GitHub and verifying the user's GitHub token is done by storing the access token as an environment variable. The function for getting the token is imported from the utils file in the project. As the API has a call limit of 5000 calls per hour, the capacity and remaining calls available, as well as the reset time, is tracked below. Each function to retrieve and export the data in the data retrieval files have functionality to pause the workflow when the user runs out of API calls. The program  then automatically sleep until the limit is renewed, and then resume the job it was completing at the time the limit ran out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the rate limit for GitHub compared to calls used, and see when the limit will reset\n",
    "# If there are less than 100 API calls left, the program will sleep until the API limit renews.\n",
    "remaining_requests, request_limit = g.rate_limiting\n",
    "print(f\"Request limit for API Calls: {request_limit}\")\n",
    "print(f\"Remaining requests for API Calls: {remaining_requests}\")\n",
    "\n",
    "limit_reset_time = g.rate_limiting_resettime\n",
    "reset_time = datetime.fromtimestamp(limit_reset_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f\"Reset time for API Calls: {reset_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Evaluating the Data Output\n",
    "\n",
    "As this framework intends to separate code and data, and because the results yielded from running a block of code will differ in time due to the changes made for the input itself, the validation process has the key focus of providing the user of the framework with an executive summery of the metrics, outliers and findings that are of significance to the redustribution process when the script is executed. As such, the findings generated in this notebook points to relationshiops and deviations that could be of importance at this moment in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to Analyse Testing Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> dict:\n",
    "    \"\"\"Load data from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The loaded data as a Python dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_versions(data: dict) -> tuple:\n",
    "    \"\"\"Check the versions provided in the data.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data containing package versions.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - ci_only_packages (list): Packages with versions specified only in the CI file.\n",
    "            - package_info_only_packages (list): Packages with versions specified only in the PackageInfo file.\n",
    "            - both_versions_packages (list): Packages with versions specified in both CI file and PackageInfo file.\n",
    "    \"\"\"\n",
    "    ci_only_packages = []\n",
    "    package_info_only_packages = []\n",
    "    both_versions_packages = []\n",
    "\n",
    "    for package, versions in data.items():\n",
    "        ci_versions = versions.get('ci_file_version', [])\n",
    "        package_info_versions = versions.get('pkginfo_version', [])\n",
    "\n",
    "        if ci_versions and not package_info_versions:\n",
    "            ci_only_packages.append(package)\n",
    "\n",
    "        elif package_info_versions and not ci_versions:\n",
    "            package_info_only_packages.append(package)\n",
    "\n",
    "        elif ci_versions and package_info_versions:\n",
    "            both_versions_packages.append(package)\n",
    "\n",
    "    return ci_only_packages, package_info_only_packages, both_versions_packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ci_and_pkg_versions(data: dict) -> list:\n",
    "    \"\"\"Compare the CI and PackageInfo testing data specifically.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data containing package versions.\n",
    "\n",
    "    Returns:\n",
    "        list: Packages where not all versions in the CI file are above that in the PackageInfo file.\n",
    "    \"\"\"\n",
    "    packages_with_mismatch = []\n",
    "\n",
    "    for package, versions in data.items():\n",
    "        ci_versions = versions.get(\"ci_file_version\", [])\n",
    "        package_version = versions.get(\"pkginfo_version\")\n",
    "\n",
    "        if ci_versions and package_version:\n",
    "            if not all(version.parse(ci) >= version.parse(package_version[0]) for ci in ci_versions):\n",
    "                packages_with_mismatch.append(package)\n",
    "\n",
    "    return packages_with_mismatch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse and Display Testing Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General Statistics on GAP Packages and Distribution\n",
    "\n",
    "Core metrics based on the general state of packages, relevant to the management of GAP from GitHub, are provided below. These numbers are helpful in providing some foundational understanding of the current state of the programming language packages, in terms of development, distribution and redistribution. This data is on a collective level, and not per individual package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executive summary on the general overview of GAP and GAP packages hosted by the GAP organisation on GitHub\n",
    "org = g.get_organization(ORG_NAME_PACKAGES)\n",
    "repos = org.get_repos(type=\"public\")\n",
    "\n",
    "# Get the total number of GAP repositories hosted by the GAP organisation on GitHub\n",
    "total_packages = repos.totalCount\n",
    "print(f\"Number of GAP packages fra GAP Respository: {total_packages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest release and version of GAP\n",
    "repo_url = \"https://api.github.com/repos/gap-system/PackageDistro/releases/latest\"\n",
    "response = requests.get(repo_url)\n",
    "latest_release = response.json()\n",
    "latest_version = latest_release.get(\"tag_name\")\n",
    "print(f\"The latest version of GAP is: {latest_version}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of GAP packages hosted elsewhere on GitHub\n",
    "# The information is attempted gathered through the web scraping technique provided by Beautiful Soup\n",
    "# NB: These numbers are only indicative and not completely accurate due to the webpage listing style, counts per parent list item\n",
    "url = \"https://gap-packages.github.io/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the section of the webpage with the packages stored elsewhere on GitHub\n",
    "section = soup.find(\"section\", id=\"main-content\")\n",
    "heading = section.find(id=\"packages-hosted-elsewhere-on-github\")\n",
    "ul = heading.find_next(\"ul\")\n",
    "\n",
    "# Do not include any child elements that are ul or li, as not to let these increase the count\n",
    "packages = ul.find_all(\"li\", recursive=False)\n",
    "count = len(packages)\n",
    "\n",
    "print(f\"Number of GAP entities or packages hosted elsewhere on GitHub: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Individual Statistics\n",
    "\n",
    "Core metrics based on the state of each package repositories relevant to the management of GAP from GitHub, are provided below. Looking into charactersitcs of packages individually, it is possible not only to compare and contrast packages to get some indication of their relative activity, but also point to individual problems, contributors that might need some help and what packages are more extensive in terms of collaboration than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the repo data from the JSON file\n",
    "data_folder = \"collected_data\"\n",
    "repo_file_path = os.path.join(data_folder, \"repo_data.json\")\n",
    "repo_data = load_data(repo_file_path)\n",
    "\n",
    "# Load monitoring data from the JSON file\n",
    "monitoring_file_path = os.path.join(data_folder, \"monitoring_data.json\")\n",
    "monitoring_data = load_data(monitoring_file_path)\n",
    "\n",
    "# Load testing data from the JSON file\n",
    "testing_file_path = os.path.join(data_folder, \"testing_data.json\")\n",
    "testing_data = load_data(testing_file_path)\n",
    "\n",
    "# Load community data from the JSON file\n",
    "community_file_path = os.path.join(data_folder, \"community_data.json\")\n",
    "# community_data = load_data(community_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Repo Data: Key Metrics and Notable Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall statistics and generate relevant insights for each repository\n",
    "total_repos = len(repo_data)\n",
    "total_releases = sum(repo['total_releases'] for repo in repo_data)\n",
    "total_open_issues = sum(repo['open_issues_count'] for repo in repo_data)\n",
    "total_open_pull_requests = sum(repo['open_pull_requests'] for repo in repo_data if repo['open_pull_requests'])\n",
    "total_bug_count = sum(repo['bug_count'] for repo in repo_data if repo['bug_count'])\n",
    "total_enhancement_count = sum(repo['enhancement_count'] for repo in repo_data if repo['enhancement_count'])\n",
    "\n",
    "# Display the calculated overall metrics\n",
    "print(f\"Total Repositories: {total_repos}\")\n",
    "print(f\"Total Releases: {total_releases}\")\n",
    "print(f\"Total Open Issues: {total_open_issues}\")\n",
    "print(f\"Total Open Pull Requests: {total_open_pull_requests}\")\n",
    "print(f\"Total Bug Count: {total_bug_count}\")\n",
    "print(f\"Total Enhancement Count: {total_enhancement_count}\")\n",
    "\n",
    "# Display inactive repositories where there has been no activity in the last 90 days\n",
    "print(\"\\nRepositories that had no activity in the last 90 days:\")\n",
    "inactive_repositories = [repo['repo'] for repo in repo_data if repo['last_activity_time'] is None]\n",
    "if inactive_repositories:\n",
    "    for repo_name in inactive_repositories:\n",
    "        print(repo_name)\n",
    "else:\n",
    "    print(\"All repositories had activity within in the past 90 days.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some other information in a ProfileReport, for more statistical analysis\n",
    "data_list = [\n",
    "    {\n",
    "        'total_releases': repo['total_releases'],\n",
    "        'age_in_days': repo['age_in_days'],\n",
    "        'open_issues_count': repo['open_issues_count'],\n",
    "        'total_pull_requests': repo['total_pull_requests'],\n",
    "        'open_pull_requests': repo['open_pull_requests'],\n",
    "        'closed_pull_requests': repo['closed_pull_requests'],\n",
    "    }\n",
    "    for repo in repo_data\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "repo_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Generate the ProfileReport based on the selected columns\n",
    "profile = ProfileReport(repo_df, title=\"Statistics for selected columns in repositories managed by gap-packages on GitHub\")\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Monitoring Data: Key Metrics and Notable Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relevant information from the loaded data\n",
    "packages_with_different_versions = monitoring_data['packages_with_different_versions']\n",
    "all_previous_and_maybe_next = monitoring_data['all_previous_and_maybe_next']\n",
    "previous_and_maybe_next_labels = monitoring_data['previous_and_maybe_next_labels']\n",
    "\n",
    "# Compare the latest released version number to the one on the main branch in GAP PackageDistro\n",
    "# Packages with different versions numbers will be in the next GAP release\n",
    "print(\"Packages with different versions in the latest GAP release and in the GAP PackageDistro:\")\n",
    "for package_data in packages_with_different_versions:\n",
    "    package_name = package_data['package_name']\n",
    "    latest_version = package_data['latest_version']\n",
    "    main_branch_version = package_data['main_branch_version']\n",
    "    print(f\"{package_name}, Latest Version: {latest_version}, Main Branch Version: {main_branch_version}\")\n",
    "\n",
    "# Find the packages in unmerged PRs, as these may be in the next release but have not yet been merged\n",
    "print(\"\\nAll packages that were in the previous release and looks to also be in the next:\")\n",
    "for package in all_previous_and_maybe_next:\n",
    "    print(package)\n",
    "\n",
    "# Only retrieve packages with unmerged PRs that have a specific labels, as these labels indicate release relation\n",
    "print(\"\\nPackages with release related labels that were in the previous release and looks to also be in the next:\")\n",
    "for package in previous_and_maybe_next_labels:\n",
    "    print(package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Data: Key Metrics and Notable Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of test directories and test files for all the repositories\n",
    "tst_dirs_with_files = len(testing_data)\n",
    "total_test_files = sum(data.get(\"tst_file_count\", 0) for data in testing_data.values())\n",
    "tst_files_info = {package: data.get(\"tst_file_count\", 0) for package, data in testing_data.items()}\n",
    "print(f\"Repositories with test directories containing files: {tst_dirs_with_files}\")\n",
    "print(f\"Total number of test files for all packages: {total_test_files}\")\n",
    "\n",
    "# Get the number of repositories with a CI.yml file, and the names of the ones who does not have one\n",
    "repos_with_ci_file = [package for package, data in testing_data.items() if \"ci_file_version\" in data]\n",
    "ci_tested_version = {package: data[\"ci_file_version\"] for package, data in testing_data.items() if \"ci_file_version\" in data}\n",
    "repos_without_ci_tests = [package for package, data in testing_data.items() if \"ci_file_version\" not in data]\n",
    "print(f\"Number of repositories with CI.yml file: {len(repos_with_ci_file)}\")\n",
    "num_packages_without_tests = len(repos_without_ci_tests)\n",
    "if num_packages_without_tests > 0:\n",
    "    print(f\"Packages without any test related data in their 'CI.yml' files: {', '.join(repos_without_ci_tests)}\")\n",
    "\n",
    "# Get the number of repositories with a PackageInfo.g file, and the names of the ones who does not have one\n",
    "repos_with_pkginfo_file = [package for package, data in testing_data.items() if \"pkginfo_version\" in data]\n",
    "repos_without_pkginfo_file = [package for package in testing_data.keys() if package not in repos_with_pkginfo_file]\n",
    "print(f\"Number of repositories with 'PackageInfo.g' file: {len(repos_with_pkginfo_file)}\")\n",
    "if len(repos_without_pkginfo_file) > 0:\n",
    "    print(f\"Packages without a 'PackageInfo.g' file: {', '.join(repos_without_pkginfo_file)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the version information for individual GAP packages\n",
    "ci_only_packages, package_info_only_packages, both_versions_packages = check_versions(testing_data)\n",
    "\n",
    "# Print the results of the analysis\n",
    "print(\"Number of packages with version testing in their CI file but not in the PackageInfo file:\", len(ci_only_packages))\n",
    "if ci_only_packages:\n",
    "    print(\"Packages with version testing for CI file but not PackageInfo file:\")\n",
    "    print(\", \".join(ci_only_packages))\n",
    "\n",
    "print(\"Number of packages with version testing in their PackageInfo file but not in the CI file:\", len(package_info_only_packages))\n",
    "if package_info_only_packages:\n",
    "    print(\"Packages with version testing for PackageInfo file but not CI file:\")\n",
    "    print(\", \".join(package_info_only_packages))\n",
    "    \n",
    "print(\"Number of packages with version testing both in the CI and PackageInfo files:\", len(both_versions_packages))\n",
    "if both_versions_packages:\n",
    "    print(\"Packages with version testing for both CI file and PackageInfo file\")\n",
    "    print(\", \".join(both_versions_packages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all version tests in the CI file are equal to or greater than the number listed in the PackageInfo file\n",
    "packages_with_mismatch = compare_ci_and_pkg_versions(testing_data)\n",
    "if packages_with_mismatch:\n",
    "    print(f\"CI versions are not all greater than or equal to PackageInfo version for package(s): {', '.join(packages_with_mismatch)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Community Data: Key Metrics and Notable Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the data from the loaded JSON file\n",
    "# Currently, the inactive contributors checks for contributors with no commits for the past 24 months\n",
    "all_authors = community_data['authors']\n",
    "all_submitters = community_data['submitters']\n",
    "author_repo_counts = community_data['author_repo_counts']\n",
    "author_submitters = community_data['author_submitters']\n",
    "inactive_contributors = community_data['inactive_contributors']\n",
    "authors_contributed_together = community_data['interactions']\n",
    "\n",
    "print(f\"Total number of authors for all GAP packages: {len(all_authors)}\")\n",
    "print(f\"Total number of submitters for all GAP packages: {len(all_submitters)}\")\n",
    "print(f\"Total number of authors who were also submitters for all GAP packages: {len(author_submitters)}\")\n",
    "print(f\"Total number of inactive contributors for all GAP packages: {len(inactive_contributors)}\")\n",
    "\n",
    "# Get information on how many repositories an author contributed to\n",
    "sorted_contributors = sorted(author_repo_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for value, count in sorted_contributors:\n",
    "    print(f\"Author Hash Value: {value}\\tRepo Contribution Count: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
